{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104491,"databundleVersionId":12585144,"sourceType":"competition"},{"sourceId":12119217,"sourceType":"datasetVersion","datasetId":7630894},{"sourceId":12122942,"sourceType":"datasetVersion","datasetId":7633493},{"sourceId":12123021,"sourceType":"datasetVersion","datasetId":7633546}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:37:50.998267Z","iopub.execute_input":"2025-06-15T17:37:50.998572Z","iopub.status.idle":"2025-06-15T17:37:51.815589Z","shell.execute_reply.started":"2025-06-15T17:37:50.998543Z","shell.execute_reply":"2025-06-15T17:37:51.814666Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/newprediction21/NewPrediction.csv\n/kaggle/input/prediction3/WhoKnowsPred.csv\n/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv\n/kaggle/input/summer-analytics-mid-hackathon/hacktrain.csv\n/kaggle/input/prediction1/Prediction.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"****Below is first model in which like missing values are replaced by overall median****","metadata":{}},{"cell_type":"code","source":"import pandas as pd \ndf=pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktrain.csv\")\ndf.fillna(df.median(numeric_only=True),inplace=True)\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\n\n# Drop ID column\ndf.drop(columns=['ID'], inplace=True)\n\n# Encode class column (if it's categorical)\nlabel_encoder = LabelEncoder()\ndf['class'] = label_encoder.fit_transform(df['class'])\n\n# Split into features and target\nX = df.drop(columns=['class'])\ny = df['class']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Fit multinomial logistic regression\nmodel = LogisticRegression(\n    multi_class='multinomial',\n    solver='lbfgs',\n    max_iter=1000\n)\nmodel.fit(X_train, y_train)\n\n# Predict on test set\ny_pred = model.predict(X_test)\n\n# Classification report with all original class labels\nprint(classification_report(\n    y_test,\n    y_pred,\n    labels=list(range(len(label_encoder.classes_))),\n    target_names=label_encoder.classes_\n))\nmodel.score(X_train,y_train)\nmodel.score(X_test,y_test)\nX_test\ntest_set=pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv\")\nids=test_set[\"ID\"]\ntest_set.drop(columns=[\"ID\"],inplace=True)\nx1=test_set\ny1=model.predict(x1)\ny1\npred_class=label_encoder.inverse_transform(y1)\nfinal_dataset=pd.DataFrame({\"ID\":ids,\"class\":pred_class})\nfinal_dataset[\"class\"].value_counts()\nfinal_dataset.to_csv(\"Prediction.csv\",index=False)\nprint(label_encoder.classes_)\ndf\nmodel.score(X_train,y_train)\n#model.score(X_test,y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:37:51.817337Z","iopub.execute_input":"2025-06-15T17:37:51.817845Z","iopub.status.idle":"2025-06-15T17:37:55.950705Z","shell.execute_reply.started":"2025-06-15T17:37:51.817814Z","shell.execute_reply":"2025-06-15T17:37:55.949814Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n        farm       0.77      0.80      0.79       168\n      forest       0.98      0.99      0.98      1232\n       grass       0.62      0.46      0.53        39\n  impervious       0.75      0.82      0.79       134\n     orchard       0.50      0.17      0.25         6\n       water       0.92      0.52      0.67        21\n\n    accuracy                           0.93      1600\n   macro avg       0.76      0.63      0.67      1600\nweighted avg       0.93      0.93      0.93      1600\n\n['farm' 'forest' 'grass' 'impervious' 'orchard' 'water']\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"0.94796875"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"****So like for second model what we tried to is like replace the missing values with median of respective values for each class and thats what worked better***","metadata":{}},{"cell_type":"code","source":"dataset=pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktrain.csv\")\ndataset1=dataset[pd.notnull(dataset[\"20150720_N\"])]\nclass_median=pd.DataFrame(dataset1.groupby(\"class\")[column].median() for column in dataset.columns.values[3:])\nclass_median.reset_index()\ndef datafix(row):\n    columns=dataset.columns.values[3:]\n    for column in columns:\n      if pd.isna(row[column]) and row[\"class\"]==\"farm\":\n          row[column]=class_median.loc[column,\"farm\"]\n      if pd.isna(row[column]) and row[\"class\"]==\"forest\":\n          row[column]=class_median.loc[column,\"forest\"]\n      if pd.isna(row[column]) and row[\"class\"]==\"grass\":\n          row[column]=class_median.loc[column,\"grass\"]\n      if pd.isna(row[column]) and row[\"class\"]==\"impervious\":\n          row[column]=class_median.loc[column,\"impervious\"]\n      if pd.isna(row[column]) and row[\"class\"]==\"water\":\n          row[column]=class_median.loc[column,\"water\"]\n      if pd.isna(row[column]) and row[\"class\"]==\"orchard\":\n          row[column]=class_median.loc[column,\"orchard\"]\n    return row\ndataset=dataset.apply(datafix,axis=\"columns\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:37:55.951576Z","iopub.execute_input":"2025-06-15T17:37:55.951828Z","iopub.status.idle":"2025-06-15T17:38:00.033839Z","shell.execute_reply.started":"2025-06-15T17:37:55.951808Z","shell.execute_reply":"2025-06-15T17:38:00.032916Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"***Standardization was also done along with better replacement of missing values***","metadata":{}},{"cell_type":"code","source":"data=dataset.copy()\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n\n# Drop ID column\n\n# Encode class column (if it's categorical)\nlabel_encoder3 = LabelEncoder()\ndata['class'] = label_encoder3.fit_transform(data['class'])\n\n# Split into features and target\nX = data.iloc[:,3:]\ny = data['class']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n# Fit multinomial logistic regression\nmodel = LogisticRegression(\n    multi_class='multinomial',\n    solver='lbfgs',\n    max_iter=2000\n)\nmodel.fit(X_train_scaled, y_train)\n# Predict on test set\ny_pred = model.predict(X_test_scaled)\n\n# Classification report with all original class labels\n'''print(classification_report(\n    y_test,\n    y_pred,\n    labels=list(range(len(label_encoder3.classes_))),\n    target_names=label_encoder3.classes_\n))'''\nmodel.score(X_train_scaled,y_train)\nmodel.score(X_test_scaled,y_test)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ny_test_decoded=label_encoder3.inverse_transform(y_test)\ny_pred_decoded=label_encoder3.inverse_transform(y_pred)\ncm=confusion_matrix(y_test_decoded,y_pred_decoded)\nplt.figure(figsize=(10, 8))\n'''sns.heatmap(cm, \n            annot=True,  # Show numbers in cells\n            fmt='d',     # Format as integers\n            cmap='Blues',\n            xticklabels=label_encoder3.classes_,\n            yticklabels=label_encoder3.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()'''\nold_pred=pd.read_csv(\"/kaggle/input/prediction1/Prediction.csv\")\nold_pred[\"class\"].value_counts()\nnew_dataset=pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv\")\nids=new_dataset[\"ID\"]\nx_new=new_dataset.iloc[:,2:]\nx_new\nx_scaled=scaler.transform(x_new)\nx_scaled\ny_new=model.predict(x_scaled)\ny_pred_new=label_encoder3.inverse_transform(y_new)\ny_pred_new\ndfinal=pd.DataFrame({\"ID\":ids,\"class\":y_pred_new})\nold_pred[\"class\"].value_counts()\ndfinal[\"class\"].value_counts()\ndfinal.to_csv(\"NewPrediction23.csv\",index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:38:00.034838Z","iopub.execute_input":"2025-06-15T17:38:00.035093Z","iopub.status.idle":"2025-06-15T17:38:01.167585Z","shell.execute_reply.started":"2025-06-15T17:38:00.035071Z","shell.execute_reply":"2025-06-15T17:38:01.166634Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 0 Axes>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"****Below is some like analysis of previous versus new predictions****","metadata":{}},{"cell_type":"code","source":"# Load both prediction files\nold_predictions = pd.read_csv(\"NewPrediction23.csv\")  # Previous prediction with score of 0.53136\nnew_predictions = pd.read_csv(\"/kaggle/input/prediction3/WhoKnowsPred.csv\")      # New prediction\n\n# Checking the different predictions\npredictions_changed = (old_predictions['class'] != new_predictions['class']).sum()\ntotal_predictions = len(old_predictions)\n\nprint(f\"Total predictions: {total_predictions}\")\nprint(f\"Changed predictions: {predictions_changed}\")\nprint(f\"Percentage changed: {predictions_changed/total_predictions*100:.2f}%\")\n\n# Loooking where they are different\ndifferences = old_predictions[old_predictions['class'] != new_predictions['class']]\nprint(f\"\\nFirst 10 differences:\")\nprint(differences[['ID']].head(10))\n\n# Also checking difference and distribution according to count\nprint(\"\\nOld model class distribution:\")\nprint(old_predictions['class'].value_counts().sort_index())\nprint(\"\\nNew model class distribution:\")\nprint(new_predictions['class'].value_counts().sort_index())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:38:01.170036Z","iopub.execute_input":"2025-06-15T17:38:01.170587Z","iopub.status.idle":"2025-06-15T17:38:01.199332Z","shell.execute_reply.started":"2025-06-15T17:38:01.170563Z","shell.execute_reply":"2025-06-15T17:38:01.198189Z"}},"outputs":[{"name":"stdout","text":"Total predictions: 2845\nChanged predictions: 2473\nPercentage changed: 86.92%\n\nFirst 10 differences:\n    ID\n0    1\n1    2\n2    3\n3    4\n4    5\n5    6\n7    8\n8    9\n9   10\n10  11\n\nOld model class distribution:\nclass\nfarm           459\nforest        1736\ngrass          123\nimpervious     418\norchard          7\nwater          102\nName: count, dtype: int64\n\nNew model class distribution:\nclass\nfarm            54\nforest         128\ngrass          324\nimpervious    2248\norchard         14\nwater           77\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Creating a comparison dataframe\ncomparison = pd.DataFrame({\n    'ID': old_predictions['ID'],\n    'old_pred': old_predictions['class'],\n    'new_pred': new_predictions['class'],\n    'changed': old_predictions['class'] != new_predictions['class']\n})\n\n# Looking at sample changes\nprint(\"Sample prediction changes:\")\nchanged_samples = comparison[comparison['changed']].head(10)\nfor _, row in changed_samples.iterrows():\n    print(f\"ID {row['ID']}: {row['old_pred']} → {row['new_pred']}\")\n\n# Checking that new model is confident in which of the classes\nprint(\"\\nClass changes summary:\")\nchange_summary = comparison[comparison['changed']].groupby(['old_pred', 'new_pred']).size()\nprint(change_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:38:01.200148Z","iopub.execute_input":"2025-06-15T17:38:01.200406Z","iopub.status.idle":"2025-06-15T17:38:01.216676Z","shell.execute_reply.started":"2025-06-15T17:38:01.200386Z","shell.execute_reply":"2025-06-15T17:38:01.215838Z"}},"outputs":[{"name":"stdout","text":"Sample prediction changes:\nID 1: farm → impervious\nID 2: forest → impervious\nID 3: orchard → impervious\nID 4: farm → grass\nID 5: orchard → impervious\nID 6: farm → grass\nID 8: forest → grass\nID 9: farm → impervious\nID 10: farm → impervious\nID 11: forest → impervious\n\nClass changes summary:\nold_pred    new_pred  \nfarm        forest           8\n            grass          121\n            impervious     325\n            water            4\nforest      farm             2\n            grass          150\n            impervious    1504\n            orchard          1\n            water           19\ngrass       forest           1\n            impervious     119\nimpervious  farm            40\n            forest          36\n            grass           32\n            orchard          4\n            water           26\norchard     grass            2\n            impervious       5\nwater       farm            11\n            forest          23\n            grass           16\n            impervious      15\n            orchard          9\ndtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(\"=== MODEL COMPARISON ===\")\nprint(f\"Old submission score: 0.53136\")\nprint(f\"New model validation accuracy: {model.score(x_scaled, y_new):.5f}\")\nprint(f\"Predictions changed: {predictions_changed}/{total_predictions}\")\n\nif model.score(X_test_scaled, y_test) > 0.53136:\n    print(\"✅ Validation accuracy improved - good sign!\")\nelse:\n    print(\"⚠️  Validation accuracy not clearly better\")\n\nif predictions_changed > total_predictions * 0.1:  # More than 10% changed\n    print(\"✅ Significant prediction changes - could be much better!\")\nelif predictions_changed > 0:\n    print(\"✅ Some changes - might be slightly better\")\nelse:\n    print(\"❌ No changes - same score expected\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:38:01.217438Z","iopub.execute_input":"2025-06-15T17:38:01.217729Z","iopub.status.idle":"2025-06-15T17:38:01.229712Z","shell.execute_reply.started":"2025-06-15T17:38:01.217706Z","shell.execute_reply":"2025-06-15T17:38:01.228853Z"}},"outputs":[{"name":"stdout","text":"=== MODEL COMPARISON ===\nOld submission score: 0.53136\nNew model validation accuracy: 1.00000\nPredictions changed: 2473/2845\n✅ Validation accuracy improved - good sign!\n✅ Significant prediction changes - could be much better!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=dataset.copy()\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import RobustScaler,PowerTransformer,MinMaxScaler,StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n\n# Drop ID column\n\n# Encode class column (if it's categorical)\nlabel_encoder3 = LabelEncoder()\ndata['class'] = label_encoder3.fit_transform(data['class'])\n\n# Split into features and target\nX = data.iloc[:,3:]\ny = data['class']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)\npowT=RobustScaler()\nX_train_pow=powT.fit_transform(X_train)\nX_test_pow=powT.transform(X_test)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_pow)\nX_test_scaled = scaler.transform(X_test_pow)\n\n# Fit multinomial logistic regression\nmodel = LogisticRegression(\n    multi_class='multinomial',\n    solver='lbfgs',\n    max_iter=2000\n)\nmodel.fit(X_train_scaled, y_train)\n# Predict on test set\ny_pred = model.predict(X_test_scaled)\n\n# Classification report with all original class labels\n'''print(classification_report(\n    y_test,\n    y_pred,\n    labels=list(range(len(label_encoder3.classes_))),\n    target_names=label_encoder3.classes_\n))'''\nmodel.score(X_train_scaled,y_train)\nmodel.score(X_test_scaled,y_test)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ny_test_decoded=label_encoder3.inverse_transform(y_test)\ny_pred_decoded=label_encoder3.inverse_transform(y_pred)\ncm=confusion_matrix(y_test_decoded,y_pred_decoded)\nplt.figure(figsize=(10, 8))\n'''sns.heatmap(cm, \n            annot=True,  # Show numbers in cells\n            fmt='d',     # Format as integers\n            cmap='Blues',\n            xticklabels=label_encoder3.classes_,\n            yticklabels=label_encoder3.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()'''\nold_pred=pd.read_csv(\"/kaggle/input/newprediction21/NewPrediction.csv\")\nold_pred[\"class\"].value_counts()\nnew_dataset=pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv\")\nids=new_dataset[\"ID\"]\nx_new=new_dataset.iloc[:,2:]\nx_new\nx_pow=powT.transform(x_new)\nx_scaled=scaler.transform(x_pow)\ny_new=model.predict(x_scaled)\ny_pred_new=label_encoder3.inverse_transform(y_new)\ny_pred_new\ndfinal=pd.DataFrame({\"ID\":ids,\"class\":y_pred_new})\nold_pred[\"class\"].value_counts()\ndfinal[\"class\"].value_counts()\n#dfinal.to_csv(\"NewPrediction23.csv\",index=False)\n#model.score(X_train_scaled,y_train)\nmodel.score(X_test_scaled,y_test)\noc=old_pred[\"class\"].value_counts()\ndfi=dfinal[\"class\"].value_counts()\ndiff=oc-dfi\ndiff\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:38:01.230760Z","iopub.execute_input":"2025-06-15T17:38:01.231087Z","iopub.status.idle":"2025-06-15T17:38:02.051345Z","shell.execute_reply.started":"2025-06-15T17:38:01.231055Z","shell.execute_reply":"2025-06-15T17:38:02.050654Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"class\nforest        0\nfarm          0\nimpervious    0\ngrass         0\nwater         0\norchard       0\nName: count, dtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 0 Axes>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=dataset.copy()\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n\n# Drop ID column\n\n# Encode class column (if it's categorical)\nlabel_encoder3 = LabelEncoder()\ndata['class'] = label_encoder3.fit_transform(data['class'])\n\n# Split into features and target\nX = data.iloc[:,3:]\ny = data['class']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n# Fit multinomial logistic regression\nmodel = LogisticRegression(\n    multi_class='multinomial',\n    solver='lbfgs',\n    max_iter=2000,\nC=1)\nmodel.fit(X_train_scaled, y_train)\n# Predict on test set\ny_pred = model.predict(X_test_scaled)\n\n# Classification report with all original class labels\n'''print(classification_report(\n    y_test,\n    y_pred,\n    labels=list(range(len(label_encoder3.classes_))),\n    target_names=label_encoder3.classes_\n))'''\nmodel.score(X_train_scaled,y_train)\nmodel.score(X_test_scaled,y_test)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ny_test_decoded=label_encoder3.inverse_transform(y_test)\ny_pred_decoded=label_encoder3.inverse_transform(y_pred)\ncm=confusion_matrix(y_test_decoded,y_pred_decoded)\nplt.figure(figsize=(10, 8))\n'''sns.heatmap(cm, \n            annot=True,  # Show numbers in cells\n            fmt='d',     # Format as integers\n            cmap='Blues',\n            xticklabels=label_encoder3.classes_,\n            yticklabels=label_encoder3.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()'''\nold_pred=pd.read_csv(\"/kaggle/input/newprediction21/NewPrediction.csv\")\nold_pred[\"class\"].value_counts()\nnew_dataset=pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv\")\nids=new_dataset[\"ID\"]\nx_new=new_dataset.iloc[:,2:]\nx_new\nx_scaled=scaler.transform(x_new)\nx_scaled\ny_new=model.predict(x_scaled)\ny_pred_new=label_encoder3.inverse_transform(y_new)\ny_pred_new\ndfinal=pd.DataFrame({\"ID\":ids,\"class\":y_pred_new})\nold_pred[\"class\"].value_counts()\ndfinal[\"class\"].value_counts()\ndfinal.to_csv(\"NewPrediction234.csv\",index=False)\nmodel.score(X_train_scaled,y_train)\nmodel.score(X_test_scaled,y_test)\ndfinal[\"class\"].value_counts()\n#old_pred[\"class\"].value_counts()\nfrom sklearn.model_selection import cross_val_score\nX_scaled=scaler.transform(X)\nscores = cross_val_score(model, X_scaled, y, cv=5)\ndfinal[\"class\"].value_counts()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:38:02.051956Z","iopub.execute_input":"2025-06-15T17:38:02.052167Z","iopub.status.idle":"2025-06-15T17:38:06.250263Z","shell.execute_reply.started":"2025-06-15T17:38:02.052150Z","shell.execute_reply":"2025-06-15T17:38:06.249648Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"class\nforest        1736\nfarm           459\nimpervious     418\ngrass          123\nwater          102\norchard          7\nName: count, dtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 0 Axes>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"training1=pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv\")\nY1=pd.read_csv(\"/kaggle/working/NewPrediction23.csv\")\ntraining1[\"class\"]=Y1[\"class\"]\ntraining12=training1.copy()\ntraining12.iloc[:,2:29]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:38:06.250827Z","iopub.execute_input":"2025-06-15T17:38:06.251043Z","iopub.status.idle":"2025-06-15T17:38:06.332843Z","shell.execute_reply.started":"2025-06-15T17:38:06.251025Z","shell.execute_reply":"2025-06-15T17:38:06.332149Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"      20150720_N  20150602_N  20150517_N  20150501_N  20150415_N  20150330_N  \\\n0      7466.4200     413.162    5761.000     5625.45    489.4030     3923.84   \n1      7235.2600    6037.350    1027.560     6085.14   1618.0500     6668.54   \n2      7425.0800    6969.980    1177.940     7408.93    861.0610     7644.43   \n3      7119.1200    1731.620    6311.930     6441.61    465.9790     7128.42   \n4      7519.5500    8130.260    1482.540     7879.53   1001.2100     7937.60   \n...          ...         ...         ...         ...         ...         ...   \n2840  -1673.7400   -2514.480   -2451.190    -2738.44     64.4464    -2275.03   \n2841    -96.8233    -412.727   -1795.400    -2363.82  -2168.1900    -2162.68   \n2842  -2364.6000    -155.592   -1422.090    -1713.40    465.6220    -2230.40   \n2843  -3004.6300   -1217.120     180.122    -1113.89    438.4180    -2442.51   \n2844  -2975.1000   -1129.790     463.748    -5355.40    193.5110    -2590.16   \n\n      20150314_N  20150226_N  20150210_N  20150125_N  ...  20140610_N  \\\n0       3097.110  6766.42000    2829.130    6742.570  ...     801.184   \n1       2513.990  1051.69000    7268.220    6908.180  ...    5533.470   \n2        814.458  1504.29000    7002.630    6086.560  ...    1981.390   \n3       1649.120  6935.22000    2176.800     690.408  ...     959.344   \n4       4122.530  1094.51000    7951.440    8001.350  ...    7636.070   \n...          ...         ...         ...         ...  ...         ...   \n2840   -2881.100 -4738.97000   -4293.570   -4402.210  ...   -2257.890   \n2841   -3155.740 -4416.11000   -5648.660   -5193.280  ...   -3991.910   \n2842   -3088.730 -5010.32000   -4211.420   -3354.430  ...   -2484.500   \n2843   -3210.560 -3237.74000    -192.857   -2857.540  ...   -3291.490   \n2844   -3113.520    -2.38883   -1248.450     328.306  ...   -3058.230   \n\n      20140525_N  20140509_N  20140423_N  20140407_N  20140322_N  20140218_N  \\\n0        927.115     4704.14     6378.42     340.949   2695.5700     527.268   \n1       5103.040     5216.12     4885.27    4366.790   1234.1400    3298.110   \n2       6204.540     7021.69     5704.41    4897.450   1789.9900    2206.100   \n3       5794.150     1045.57     5572.90     586.287    685.9060    1287.000   \n4       6996.760     7413.43     4596.13    4511.700   1413.5200    3283.940   \n...          ...         ...         ...         ...         ...         ...   \n2840   -2582.420    -1699.10    -2295.30     306.483    209.4030     221.920   \n2841   -2614.910    -2109.84    -2687.18   -2853.890     71.3402     -86.414   \n2842   -1756.080    -3820.43    -1988.23     433.852    291.8000     254.548   \n2843   -2018.450    -3472.65    -3109.07     511.792    369.0970     276.948   \n2844   -2276.180    -3335.00    -2929.19   -1007.500    271.0580     307.997   \n\n      20140202_N  20140117_N  20140101_N  \n0        4736.75     601.843    6639.760  \n1        6942.68    1070.440     842.101  \n2        6928.93    1036.560     831.441  \n3        6734.72     824.584    6883.610  \n4        7937.68    1857.800    1336.920  \n...          ...         ...         ...  \n2840    -4885.15   -2630.590     356.233  \n2841    -5109.01     324.637   -4316.580  \n2842    -4259.30     412.115   -1170.750  \n2843    -5574.51     298.739     460.419  \n2844    -5453.33     307.495     417.124  \n\n[2845 rows x 27 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>20150720_N</th>\n      <th>20150602_N</th>\n      <th>20150517_N</th>\n      <th>20150501_N</th>\n      <th>20150415_N</th>\n      <th>20150330_N</th>\n      <th>20150314_N</th>\n      <th>20150226_N</th>\n      <th>20150210_N</th>\n      <th>20150125_N</th>\n      <th>...</th>\n      <th>20140610_N</th>\n      <th>20140525_N</th>\n      <th>20140509_N</th>\n      <th>20140423_N</th>\n      <th>20140407_N</th>\n      <th>20140322_N</th>\n      <th>20140218_N</th>\n      <th>20140202_N</th>\n      <th>20140117_N</th>\n      <th>20140101_N</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7466.4200</td>\n      <td>413.162</td>\n      <td>5761.000</td>\n      <td>5625.45</td>\n      <td>489.4030</td>\n      <td>3923.84</td>\n      <td>3097.110</td>\n      <td>6766.42000</td>\n      <td>2829.130</td>\n      <td>6742.570</td>\n      <td>...</td>\n      <td>801.184</td>\n      <td>927.115</td>\n      <td>4704.14</td>\n      <td>6378.42</td>\n      <td>340.949</td>\n      <td>2695.5700</td>\n      <td>527.268</td>\n      <td>4736.75</td>\n      <td>601.843</td>\n      <td>6639.760</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7235.2600</td>\n      <td>6037.350</td>\n      <td>1027.560</td>\n      <td>6085.14</td>\n      <td>1618.0500</td>\n      <td>6668.54</td>\n      <td>2513.990</td>\n      <td>1051.69000</td>\n      <td>7268.220</td>\n      <td>6908.180</td>\n      <td>...</td>\n      <td>5533.470</td>\n      <td>5103.040</td>\n      <td>5216.12</td>\n      <td>4885.27</td>\n      <td>4366.790</td>\n      <td>1234.1400</td>\n      <td>3298.110</td>\n      <td>6942.68</td>\n      <td>1070.440</td>\n      <td>842.101</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7425.0800</td>\n      <td>6969.980</td>\n      <td>1177.940</td>\n      <td>7408.93</td>\n      <td>861.0610</td>\n      <td>7644.43</td>\n      <td>814.458</td>\n      <td>1504.29000</td>\n      <td>7002.630</td>\n      <td>6086.560</td>\n      <td>...</td>\n      <td>1981.390</td>\n      <td>6204.540</td>\n      <td>7021.69</td>\n      <td>5704.41</td>\n      <td>4897.450</td>\n      <td>1789.9900</td>\n      <td>2206.100</td>\n      <td>6928.93</td>\n      <td>1036.560</td>\n      <td>831.441</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7119.1200</td>\n      <td>1731.620</td>\n      <td>6311.930</td>\n      <td>6441.61</td>\n      <td>465.9790</td>\n      <td>7128.42</td>\n      <td>1649.120</td>\n      <td>6935.22000</td>\n      <td>2176.800</td>\n      <td>690.408</td>\n      <td>...</td>\n      <td>959.344</td>\n      <td>5794.150</td>\n      <td>1045.57</td>\n      <td>5572.90</td>\n      <td>586.287</td>\n      <td>685.9060</td>\n      <td>1287.000</td>\n      <td>6734.72</td>\n      <td>824.584</td>\n      <td>6883.610</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7519.5500</td>\n      <td>8130.260</td>\n      <td>1482.540</td>\n      <td>7879.53</td>\n      <td>1001.2100</td>\n      <td>7937.60</td>\n      <td>4122.530</td>\n      <td>1094.51000</td>\n      <td>7951.440</td>\n      <td>8001.350</td>\n      <td>...</td>\n      <td>7636.070</td>\n      <td>6996.760</td>\n      <td>7413.43</td>\n      <td>4596.13</td>\n      <td>4511.700</td>\n      <td>1413.5200</td>\n      <td>3283.940</td>\n      <td>7937.68</td>\n      <td>1857.800</td>\n      <td>1336.920</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2840</th>\n      <td>-1673.7400</td>\n      <td>-2514.480</td>\n      <td>-2451.190</td>\n      <td>-2738.44</td>\n      <td>64.4464</td>\n      <td>-2275.03</td>\n      <td>-2881.100</td>\n      <td>-4738.97000</td>\n      <td>-4293.570</td>\n      <td>-4402.210</td>\n      <td>...</td>\n      <td>-2257.890</td>\n      <td>-2582.420</td>\n      <td>-1699.10</td>\n      <td>-2295.30</td>\n      <td>306.483</td>\n      <td>209.4030</td>\n      <td>221.920</td>\n      <td>-4885.15</td>\n      <td>-2630.590</td>\n      <td>356.233</td>\n    </tr>\n    <tr>\n      <th>2841</th>\n      <td>-96.8233</td>\n      <td>-412.727</td>\n      <td>-1795.400</td>\n      <td>-2363.82</td>\n      <td>-2168.1900</td>\n      <td>-2162.68</td>\n      <td>-3155.740</td>\n      <td>-4416.11000</td>\n      <td>-5648.660</td>\n      <td>-5193.280</td>\n      <td>...</td>\n      <td>-3991.910</td>\n      <td>-2614.910</td>\n      <td>-2109.84</td>\n      <td>-2687.18</td>\n      <td>-2853.890</td>\n      <td>71.3402</td>\n      <td>-86.414</td>\n      <td>-5109.01</td>\n      <td>324.637</td>\n      <td>-4316.580</td>\n    </tr>\n    <tr>\n      <th>2842</th>\n      <td>-2364.6000</td>\n      <td>-155.592</td>\n      <td>-1422.090</td>\n      <td>-1713.40</td>\n      <td>465.6220</td>\n      <td>-2230.40</td>\n      <td>-3088.730</td>\n      <td>-5010.32000</td>\n      <td>-4211.420</td>\n      <td>-3354.430</td>\n      <td>...</td>\n      <td>-2484.500</td>\n      <td>-1756.080</td>\n      <td>-3820.43</td>\n      <td>-1988.23</td>\n      <td>433.852</td>\n      <td>291.8000</td>\n      <td>254.548</td>\n      <td>-4259.30</td>\n      <td>412.115</td>\n      <td>-1170.750</td>\n    </tr>\n    <tr>\n      <th>2843</th>\n      <td>-3004.6300</td>\n      <td>-1217.120</td>\n      <td>180.122</td>\n      <td>-1113.89</td>\n      <td>438.4180</td>\n      <td>-2442.51</td>\n      <td>-3210.560</td>\n      <td>-3237.74000</td>\n      <td>-192.857</td>\n      <td>-2857.540</td>\n      <td>...</td>\n      <td>-3291.490</td>\n      <td>-2018.450</td>\n      <td>-3472.65</td>\n      <td>-3109.07</td>\n      <td>511.792</td>\n      <td>369.0970</td>\n      <td>276.948</td>\n      <td>-5574.51</td>\n      <td>298.739</td>\n      <td>460.419</td>\n    </tr>\n    <tr>\n      <th>2844</th>\n      <td>-2975.1000</td>\n      <td>-1129.790</td>\n      <td>463.748</td>\n      <td>-5355.40</td>\n      <td>193.5110</td>\n      <td>-2590.16</td>\n      <td>-3113.520</td>\n      <td>-2.38883</td>\n      <td>-1248.450</td>\n      <td>328.306</td>\n      <td>...</td>\n      <td>-3058.230</td>\n      <td>-2276.180</td>\n      <td>-3335.00</td>\n      <td>-2929.19</td>\n      <td>-1007.500</td>\n      <td>271.0580</td>\n      <td>307.997</td>\n      <td>-5453.33</td>\n      <td>307.495</td>\n      <td>417.124</td>\n    </tr>\n  </tbody>\n</table>\n<p>2845 rows × 27 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"training12=training1.copy()\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n\n\ndata=training12.copy()\n# Encode class column (if it's categorical)\nlabel_encoder3 = LabelEncoder()\ndata['class'] = label_encoder3.fit_transform(data['class'])\n\n# Split into features and target\nX = data.iloc[:,2:29]\ny = data['class']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.95, random_state=42, stratify=y)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n# Fit multinomial logistic regression\nmodel = LogisticRegression(\n    multi_class='multinomial',\n    solver='lbfgs',\n    max_iter=2000,\nC=1)\nmodel.fit(X_train_scaled, y_train)\n# Predict on test set\ny_pred = model.predict(X_test_scaled)\n\n# Classification report with all original class labels\n'''print(classification_report(\n    y_test,\n    y_pred,\n    labels=list(range(len(label_encoder3.classes_))),\n    target_names=label_encoder3.classes_\n))'''\nmodel.score(X_train_scaled,y_train)\nmodel.score(X_test_scaled,y_test)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ny_test_decoded=label_encoder3.inverse_transform(y_test)\ny_pred_decoded=label_encoder3.inverse_transform(y_pred)\ncm=confusion_matrix(y_test_decoded,y_pred_decoded)\nplt.figure(figsize=(10, 8))\n'''sns.heatmap(cm, \n            annot=True,  # Show numbers in cells\n            fmt='d',     # Format as integers\n            cmap='Blues',\n            xticklabels=label_encoder3.classes_,\n            yticklabels=label_encoder3.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()'''\nmodel.score(X_train_scaled,y_train)\nmodel.score(X_test_scaled,y_test)\nnew_data1=pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv\")\nx12=new_data1.iloc[:,2:]\nx12_scaled=scaler.transform(x12)\ny_pred12=model.predict(x12)\ny_final=label_encoder3.inverse_transform(y_pred12)\nY1[\"class\"].value_counts()\nY2=pd.read_csv(\"/kaggle/working/NewPrediction23.csv\")\nids=Y1[\"ID\"]\nnew_data1=pd.DataFrame({\"ID\":ids,\"class\":y_final})\nnd=new_data1[\"class\"].value_counts()\nyy=Y2[\"class\"].value_counts()\ndiff=nd-yy\ndiff\n#new_data1.to_csv(\"Prediction4.csv\",index=False)\nnew_data1[\"class\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:38:06.333465Z","iopub.execute_input":"2025-06-15T17:38:06.333719Z","iopub.status.idle":"2025-06-15T17:38:06.486309Z","shell.execute_reply.started":"2025-06-15T17:38:06.333698Z","shell.execute_reply":"2025-06-15T17:38:06.485648Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n  warnings.warn(\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"class\nforest        2794\nwater           28\nfarm            10\nimpervious       7\ngrass            6\nName: count, dtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 0 Axes>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"new_data1=pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv\")\nx12=new_data1.iloc[:,2:]\nx12_scaled=scaler.transform(x12)\ny_pred12=model.predict(x12)\ny_final=label_encoder3.inverse_transform(y_pred12)\nmodel.score(x12_scaled,Y1[\"class\"])\nY1[\"class\"].count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:38:06.486940Z","iopub.execute_input":"2025-06-15T17:38:06.487183Z","iopub.status.idle":"2025-06-15T17:38:06.529471Z","shell.execute_reply.started":"2025-06-15T17:38:06.487162Z","shell.execute_reply":"2025-06-15T17:38:06.528114Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n  warnings.warn(\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"2845"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"def robust_preprocessing(df):\n    \"\"\"Additional techniques for handling noisy data\"\"\"\n    \n    # 1. Median filtering for time series smoothing\n    from scipy.signal import medfilt\n    \n    ndvi_cols = [col for col in df.columns if col.endswith('_N')]\n    df_smooth = df.copy()\n    \n    for idx in df.index:\n        ts_data = df.loc[idx, ndvi_cols].values\n        # Apply median filter to smooth noise\n        if not np.isnan(ts_data).all():\n            smoothed = medfilt(ts_data, kernel_size=3)\n            df_smooth.loc[idx, ndvi_cols] = smoothed\n    \n    return df_smooth\n#print(robust_preprocessing(dataset))\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:38:06.530408Z","iopub.execute_input":"2025-06-15T17:38:06.530799Z","iopub.status.idle":"2025-06-15T17:38:06.566353Z","shell.execute_reply.started":"2025-06-15T17:38:06.530770Z","shell.execute_reply":"2025-06-15T17:38:06.565521Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"      Unnamed: 0     ID       class  20150720_N  20150602_N  20150517_N  \\\n0              0      1       water    637.5950     658.668   -1882.030   \n1              1      2       water    634.2400     593.705   -1625.790   \n2              3      4       water     58.0174   -1599.160    -148.529   \n3              4      5       water     72.5180     750.756     380.436   \n4              7      8       water   1136.4400     750.756    -148.529   \n...          ...    ...         ...         ...         ...         ...   \n7995       10537  10538  impervious   1207.7000     984.620    1103.030   \n7996       10538  10539  impervious   2170.3500    1419.720    1361.000   \n7997       10541  10542  impervious   1895.6800    1454.740    1103.030   \n7998       10542  10543  impervious   3465.7400    1283.320     413.412   \n7999       10544  10545  impervious   6941.1900    1667.870    5084.780   \n\n      20150501_N  20150415_N  20150330_N  20150314_N  ...  20140610_N  \\\n0       -1924.36     997.904  -1739.9900     630.087  ...    1537.140   \n1       -1672.32     914.198   -692.3860     707.626  ...    1537.140   \n2       -1052.63     458.498  -1564.6300     753.058  ...   -1025.880   \n3       -1256.93     515.805  -1413.1800    -802.942  ...   -1813.950   \n4        1647.83    1935.800    512.6525    2158.980  ...    1535.000   \n...          ...         ...         ...         ...  ...         ...   \n7995     1166.25     937.478   1072.7000     823.896  ...    1117.740   \n7996     1478.71     983.911   1262.1100    1422.860  ...     984.634   \n7997     1033.56    1930.380   1057.1500    1471.600  ...     888.408   \n7998     4391.05    1146.820   4473.0500    1614.750  ...    5833.760   \n7999     1713.13    1588.950   5978.1900    1483.755  ...    7352.570   \n\n      20140525_N  20140509_N  20140423_N  20140407_N  20140322_N  20140218_N  \\\n0      -1043.160   -1942.490     267.138     754.153     764.003     211.328   \n1       -933.934    -625.385     120.059     364.858     476.972     220.878   \n2        368.622     821.745   -1227.800     304.621     764.003     369.214   \n3        155.624     821.745    -924.073     432.150     282.833     298.320   \n4       1959.430    -279.317    -384.915    -113.406    1020.720    1660.650   \n...          ...         ...         ...         ...         ...         ...   \n7995    1176.600    1044.110    1178.140     369.082     465.843     362.882   \n7996    2128.970    1379.660    1178.140     762.633     485.204     446.724   \n7997    2093.020    1232.110    1190.830    1441.460    1170.880    1095.000   \n7998    4047.320    4515.800     433.177     277.296     744.143     484.877   \n7999    3289.860    3729.150    1994.980    1003.085    5299.900     484.877   \n\n      20140202_N  20140117_N  20140101_N  \n0      -2203.020   -1180.190     433.906  \n1      -2250.000   -1360.560     524.075  \n2      -2202.120     804.372   -1343.550  \n3      -2197.360     804.372    -826.727  \n4       -116.801    -568.050   -1357.140  \n...          ...         ...         ...  \n7995     979.795    1240.080     433.659  \n7996     771.747    1589.060     506.936  \n7997    1818.650    2501.720    1247.770  \n7998    3759.710    1240.080     388.346  \n7999    5983.130    1249.710    2424.230  \n\n[8000 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>ID</th>\n      <th>class</th>\n      <th>20150720_N</th>\n      <th>20150602_N</th>\n      <th>20150517_N</th>\n      <th>20150501_N</th>\n      <th>20150415_N</th>\n      <th>20150330_N</th>\n      <th>20150314_N</th>\n      <th>...</th>\n      <th>20140610_N</th>\n      <th>20140525_N</th>\n      <th>20140509_N</th>\n      <th>20140423_N</th>\n      <th>20140407_N</th>\n      <th>20140322_N</th>\n      <th>20140218_N</th>\n      <th>20140202_N</th>\n      <th>20140117_N</th>\n      <th>20140101_N</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>water</td>\n      <td>637.5950</td>\n      <td>658.668</td>\n      <td>-1882.030</td>\n      <td>-1924.36</td>\n      <td>997.904</td>\n      <td>-1739.9900</td>\n      <td>630.087</td>\n      <td>...</td>\n      <td>1537.140</td>\n      <td>-1043.160</td>\n      <td>-1942.490</td>\n      <td>267.138</td>\n      <td>754.153</td>\n      <td>764.003</td>\n      <td>211.328</td>\n      <td>-2203.020</td>\n      <td>-1180.190</td>\n      <td>433.906</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>water</td>\n      <td>634.2400</td>\n      <td>593.705</td>\n      <td>-1625.790</td>\n      <td>-1672.32</td>\n      <td>914.198</td>\n      <td>-692.3860</td>\n      <td>707.626</td>\n      <td>...</td>\n      <td>1537.140</td>\n      <td>-933.934</td>\n      <td>-625.385</td>\n      <td>120.059</td>\n      <td>364.858</td>\n      <td>476.972</td>\n      <td>220.878</td>\n      <td>-2250.000</td>\n      <td>-1360.560</td>\n      <td>524.075</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4</td>\n      <td>water</td>\n      <td>58.0174</td>\n      <td>-1599.160</td>\n      <td>-148.529</td>\n      <td>-1052.63</td>\n      <td>458.498</td>\n      <td>-1564.6300</td>\n      <td>753.058</td>\n      <td>...</td>\n      <td>-1025.880</td>\n      <td>368.622</td>\n      <td>821.745</td>\n      <td>-1227.800</td>\n      <td>304.621</td>\n      <td>764.003</td>\n      <td>369.214</td>\n      <td>-2202.120</td>\n      <td>804.372</td>\n      <td>-1343.550</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>5</td>\n      <td>water</td>\n      <td>72.5180</td>\n      <td>750.756</td>\n      <td>380.436</td>\n      <td>-1256.93</td>\n      <td>515.805</td>\n      <td>-1413.1800</td>\n      <td>-802.942</td>\n      <td>...</td>\n      <td>-1813.950</td>\n      <td>155.624</td>\n      <td>821.745</td>\n      <td>-924.073</td>\n      <td>432.150</td>\n      <td>282.833</td>\n      <td>298.320</td>\n      <td>-2197.360</td>\n      <td>804.372</td>\n      <td>-826.727</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>8</td>\n      <td>water</td>\n      <td>1136.4400</td>\n      <td>750.756</td>\n      <td>-148.529</td>\n      <td>1647.83</td>\n      <td>1935.800</td>\n      <td>512.6525</td>\n      <td>2158.980</td>\n      <td>...</td>\n      <td>1535.000</td>\n      <td>1959.430</td>\n      <td>-279.317</td>\n      <td>-384.915</td>\n      <td>-113.406</td>\n      <td>1020.720</td>\n      <td>1660.650</td>\n      <td>-116.801</td>\n      <td>-568.050</td>\n      <td>-1357.140</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>10537</td>\n      <td>10538</td>\n      <td>impervious</td>\n      <td>1207.7000</td>\n      <td>984.620</td>\n      <td>1103.030</td>\n      <td>1166.25</td>\n      <td>937.478</td>\n      <td>1072.7000</td>\n      <td>823.896</td>\n      <td>...</td>\n      <td>1117.740</td>\n      <td>1176.600</td>\n      <td>1044.110</td>\n      <td>1178.140</td>\n      <td>369.082</td>\n      <td>465.843</td>\n      <td>362.882</td>\n      <td>979.795</td>\n      <td>1240.080</td>\n      <td>433.659</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>10538</td>\n      <td>10539</td>\n      <td>impervious</td>\n      <td>2170.3500</td>\n      <td>1419.720</td>\n      <td>1361.000</td>\n      <td>1478.71</td>\n      <td>983.911</td>\n      <td>1262.1100</td>\n      <td>1422.860</td>\n      <td>...</td>\n      <td>984.634</td>\n      <td>2128.970</td>\n      <td>1379.660</td>\n      <td>1178.140</td>\n      <td>762.633</td>\n      <td>485.204</td>\n      <td>446.724</td>\n      <td>771.747</td>\n      <td>1589.060</td>\n      <td>506.936</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>10541</td>\n      <td>10542</td>\n      <td>impervious</td>\n      <td>1895.6800</td>\n      <td>1454.740</td>\n      <td>1103.030</td>\n      <td>1033.56</td>\n      <td>1930.380</td>\n      <td>1057.1500</td>\n      <td>1471.600</td>\n      <td>...</td>\n      <td>888.408</td>\n      <td>2093.020</td>\n      <td>1232.110</td>\n      <td>1190.830</td>\n      <td>1441.460</td>\n      <td>1170.880</td>\n      <td>1095.000</td>\n      <td>1818.650</td>\n      <td>2501.720</td>\n      <td>1247.770</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>10542</td>\n      <td>10543</td>\n      <td>impervious</td>\n      <td>3465.7400</td>\n      <td>1283.320</td>\n      <td>413.412</td>\n      <td>4391.05</td>\n      <td>1146.820</td>\n      <td>4473.0500</td>\n      <td>1614.750</td>\n      <td>...</td>\n      <td>5833.760</td>\n      <td>4047.320</td>\n      <td>4515.800</td>\n      <td>433.177</td>\n      <td>277.296</td>\n      <td>744.143</td>\n      <td>484.877</td>\n      <td>3759.710</td>\n      <td>1240.080</td>\n      <td>388.346</td>\n    </tr>\n    <tr>\n      <th>7999</th>\n      <td>10544</td>\n      <td>10545</td>\n      <td>impervious</td>\n      <td>6941.1900</td>\n      <td>1667.870</td>\n      <td>5084.780</td>\n      <td>1713.13</td>\n      <td>1588.950</td>\n      <td>5978.1900</td>\n      <td>1483.755</td>\n      <td>...</td>\n      <td>7352.570</td>\n      <td>3289.860</td>\n      <td>3729.150</td>\n      <td>1994.980</td>\n      <td>1003.085</td>\n      <td>5299.900</td>\n      <td>484.877</td>\n      <td>5983.130</td>\n      <td>1249.710</td>\n      <td>2424.230</td>\n    </tr>\n  </tbody>\n</table>\n<p>8000 rows × 30 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.signal import medfilt\n\ndef robust_preprocessing(df):\n    \"\"\"Additional techniques for handling noisy data\"\"\"\n    \n    # Check if input is DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f\"Expected DataFrame, got {type(df)}. Make sure you pass a DataFrame, not a Series!\")\n    \n    # Find NDVI columns\n    ndvi_cols = [col for col in df.columns if col.endswith('_N')]\n    \n    # Check if NDVI columns exist\n    if not ndvi_cols:\n        print(\"Warning: No columns ending with '_N' found!\")\n        print(\"Available columns:\", list(df.columns))\n        return df\n    \n    print(f\"Found {len(ndvi_cols)} NDVI columns\")\n    df_smooth = df.copy()\n    \n    # Process each row\n    for idx in df.index:\n        try:\n            # Get NDVI time series for this row\n            ts_data = df.loc[idx, ndvi_cols].values\n            \n            # Check if we have valid data (not all NaN)\n            if not np.isnan(ts_data).all():\n                # Handle NaN values before median filtering\n                if np.isnan(ts_data).any():\n                    # Simple interpolation for NaN values\n                    ts_data = pd.Series(ts_data).interpolate().values\n                \n                # Apply median filter only if we have enough valid points\n                if len(ts_data) >= 3:\n                    smoothed = medfilt(ts_data, kernel_size=3)\n                    df_smooth.loc[idx, ndvi_cols] = smoothed\n                    \n        except Exception as e:\n            print(f\"Error processing row {idx}: {e}\")\n            continue\n    \n    return df_smooth\ndataset5=robust_preprocessing(dataset.iloc[:,3:])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:38:06.568635Z","iopub.execute_input":"2025-06-15T17:38:06.568903Z","iopub.status.idle":"2025-06-15T17:38:13.531623Z","shell.execute_reply.started":"2025-06-15T17:38:06.568884Z","shell.execute_reply":"2025-06-15T17:38:13.530663Z"}},"outputs":[{"name":"stdout","text":"Found 27 NDVI columns\n","output_type":"stream"}],"execution_count":14}]}