{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104491,"databundleVersionId":12585144,"sourceType":"competition"},{"sourceId":12119217,"sourceType":"datasetVersion","datasetId":7630894}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-10T10:54:31.068130Z","iopub.execute_input":"2025-06-10T10:54:31.068421Z","iopub.status.idle":"2025-06-10T10:54:31.428870Z","shell.execute_reply.started":"2025-06-10T10:54:31.068398Z","shell.execute_reply":"2025-06-10T10:54:31.427925Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv\n/kaggle/input/summer-analytics-mid-hackathon/hacktrain.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"****Below is first model in which like missing values are replaced by overall median****","metadata":{}},{"cell_type":"code","source":"import pandas as pd \ndf=pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktrain.csv\")\ndf.fillna(df.median(numeric_only=True),inplace=True)\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\n\n# Drop ID column\ndf.drop(columns=['ID'], inplace=True)\n\n# Encode class column (if it's categorical)\nlabel_encoder = LabelEncoder()\ndf['class'] = label_encoder.fit_transform(df['class'])\n\n# Split into features and target\nX = df.drop(columns=['class'])\ny = df['class']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Fit multinomial logistic regression\nmodel = LogisticRegression(\n    multi_class='multinomial',\n    solver='lbfgs',\n    max_iter=1000\n)\nmodel.fit(X_train, y_train)\n\n# Predict on test set\ny_pred = model.predict(X_test)\n\n# Classification report with all original class labels\nprint(classification_report(\n    y_test,\n    y_pred,\n    labels=list(range(len(label_encoder.classes_))),\n    target_names=label_encoder.classes_\n))\nmodel.score(X_train,y_train)\nmodel.score(X_test,y_test)\nX_test\ntest_set=pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv\")\nids=test_set[\"ID\"]\ntest_set.drop(columns=[\"ID\"],inplace=True)\nx1=test_set\ny1=model.predict(x1)\ny1\npred_class=label_encoder.inverse_transform(y1)\nfinal_dataset=pd.DataFrame({\"ID\":ids,\"class\":pred_class})\nfinal_dataset[\"class\"].value_counts()\nfinal_dataset.to_csv(\"Prediction.csv\",index=False)\nprint(label_encoder.classes_)\ndf\nmodel.score(X_train,y_train)\n#model.score(X_test,y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:49:22.337611Z","iopub.execute_input":"2025-06-10T12:49:22.338001Z","iopub.status.idle":"2025-06-10T12:49:23.880790Z","shell.execute_reply.started":"2025-06-10T12:49:22.337975Z","shell.execute_reply":"2025-06-10T12:49:23.879699Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n        farm       0.77      0.80      0.79       168\n      forest       0.98      0.99      0.98      1232\n       grass       0.62      0.46      0.53        39\n  impervious       0.75      0.82      0.79       134\n     orchard       0.50      0.17      0.25         6\n       water       0.92      0.52      0.67        21\n\n    accuracy                           0.93      1600\n   macro avg       0.76      0.63      0.67      1600\nweighted avg       0.93      0.93      0.93      1600\n\n['farm' 'forest' 'grass' 'impervious' 'orchard' 'water']\n","output_type":"stream"},{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"0.94796875"},"metadata":{}}],"execution_count":87},{"cell_type":"markdown","source":"****So like for second model what we tried to is like replace the missing values with median of respective values for each class and thats what worked better***","metadata":{}},{"cell_type":"code","source":"dataset=pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktrain.csv\")\ndataset1=dataset[pd.notnull(dataset[\"20150720_N\"])]\nclass_median=pd.DataFrame(dataset1.groupby(\"class\")[column].median() for column in dataset.columns.values[3:])\nclass_median.reset_index()\ndef datafix(row):\n    columns=dataset.columns.values[3:]\n    for column in columns:\n      if pd.isna(row[column]) and row[\"class\"]==\"farm\":\n          row[column]=class_median.loc[column,\"farm\"]\n      if pd.isna(row[column]) and row[\"class\"]==\"forest\":\n          row[column]=class_median.loc[column,\"forest\"]\n      if pd.isna(row[column]) and row[\"class\"]==\"grass\":\n          row[column]=class_median.loc[column,\"grass\"]\n      if pd.isna(row[column]) and row[\"class\"]==\"impervious\":\n          row[column]=class_median.loc[column,\"impervious\"]\n      if pd.isna(row[column]) and row[\"class\"]==\"water\":\n          row[column]=class_median.loc[column,\"water\"]\n      if pd.isna(row[column]) and row[\"class\"]==\"orchard\":\n          row[column]=class_median.loc[column,\"orchard\"]\n    return row\ndataset=dataset.apply(datafix,axis=\"columns\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:22:14.594441Z","iopub.execute_input":"2025-06-10T12:22:14.595349Z","iopub.status.idle":"2025-06-10T12:22:18.727683Z","shell.execute_reply.started":"2025-06-10T12:22:14.595317Z","shell.execute_reply":"2025-06-10T12:22:18.726756Z"}},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":"***Standardization was also done along with better replacement of missing values***","metadata":{}},{"cell_type":"code","source":"data=dataset.copy()\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n\n# Drop ID column\n\n# Encode class column (if it's categorical)\nlabel_encoder3 = LabelEncoder()\ndata['class'] = label_encoder3.fit_transform(data['class'])\n\n# Split into features and target\nX = data.iloc[:,3:]\ny = data['class']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n# Fit multinomial logistic regression\nmodel = LogisticRegression(\n    multi_class='multinomial',\n    solver='lbfgs',\n    max_iter=2000\n)\nmodel.fit(X_train_scaled, y_train)\n# Predict on test set\ny_pred = model.predict(X_test_scaled)\n\n# Classification report with all original class labels\n'''print(classification_report(\n    y_test,\n    y_pred,\n    labels=list(range(len(label_encoder3.classes_))),\n    target_names=label_encoder3.classes_\n))'''\nmodel.score(X_train_scaled,y_train)\nmodel.score(X_test_scaled,y_test)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ny_test_decoded=label_encoder3.inverse_transform(y_test)\ny_pred_decoded=label_encoder3.inverse_transform(y_pred)\ncm=confusion_matrix(y_test_decoded,y_pred_decoded)\nplt.figure(figsize=(10, 8))\n'''sns.heatmap(cm, \n            annot=True,  # Show numbers in cells\n            fmt='d',     # Format as integers\n            cmap='Blues',\n            xticklabels=label_encoder3.classes_,\n            yticklabels=label_encoder3.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()'''\nold_pred=pd.read_csv(\"/kaggle/input/prediction1/Prediction.csv\")\nold_pred[\"class\"].value_counts()\nnew_dataset=pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv\")\nids=new_dataset[\"ID\"]\nx_new=new_dataset.iloc[:,2:]\nx_new\nx_scaled=scaler.transform(x_new)\nx_scaled\ny_new=model.predict(x_scaled)\ny_pred_new=label_encoder3.inverse_transform(y_new)\ny_pred_new\ndfinal=pd.DataFrame({\"ID\":ids,\"class\":y_pred_new})\nold_pred[\"class\"].value_counts()\ndfinal[\"class\"].value_counts()\ndfinal.to_csv(\"NewPrediction.csv\",index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:55:57.855443Z","iopub.execute_input":"2025-06-10T12:55:57.855884Z","iopub.status.idle":"2025-06-10T12:55:59.203035Z","shell.execute_reply.started":"2025-06-10T12:55:57.855848Z","shell.execute_reply":"2025-06-10T12:55:59.202184Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 0 Axes>"},"metadata":{}}],"execution_count":89},{"cell_type":"markdown","source":"****Below is some like analysis of previous versus new predictions****","metadata":{}},{"cell_type":"code","source":"# Load both prediction files\nold_predictions = pd.read_csv(\"Prediction.csv\")  # Previous prediction with score of 0.53136\nnew_predictions = pd.read_csv(\"NewPrediction.csv\")      # New prediction\n\n# Checking the different predictions\npredictions_changed = (old_predictions['class'] != new_predictions['class']).sum()\ntotal_predictions = len(old_predictions)\n\nprint(f\"Total predictions: {total_predictions}\")\nprint(f\"Changed predictions: {predictions_changed}\")\nprint(f\"Percentage changed: {predictions_changed/total_predictions*100:.2f}%\")\n\n# Loooking where they are different\ndifferences = old_predictions[old_predictions['class'] != new_predictions['class']]\nprint(f\"\\nFirst 10 differences:\")\nprint(differences[['ID']].head(10))\n\n# Also checking difference and distribution according to count\nprint(\"\\nOld model class distribution:\")\nprint(old_predictions['class'].value_counts().sort_index())\nprint(\"\\nNew model class distribution:\")\nprint(new_predictions['class'].value_counts().sort_index())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:58:02.285808Z","iopub.execute_input":"2025-06-10T12:58:02.286133Z","iopub.status.idle":"2025-06-10T12:58:02.308946Z","shell.execute_reply.started":"2025-06-10T12:58:02.286109Z","shell.execute_reply":"2025-06-10T12:58:02.307680Z"}},"outputs":[{"name":"stdout","text":"Total predictions: 2845\nChanged predictions: 1017\nPercentage changed: 35.75%\n\nFirst 10 differences:\n    ID\n0    1\n2    3\n3    4\n4    5\n5    6\n8    9\n9   10\n15  16\n16  17\n18  19\n\nOld model class distribution:\nclass\nfarm             8\nforest        2588\ngrass            1\nimpervious      27\norchard          2\nwater          219\nName: count, dtype: int64\n\nNew model class distribution:\nclass\nfarm           459\nforest        1736\ngrass          123\nimpervious     418\norchard          7\nwater          102\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"# Creating a comparison dataframe\ncomparison = pd.DataFrame({\n    'ID': old_predictions['ID'],\n    'old_pred': old_predictions['class'],\n    'new_pred': new_predictions['class'],\n    'changed': old_predictions['class'] != new_predictions['class']\n})\n\n# Looking at sample changes\nprint(\"Sample prediction changes:\")\nchanged_samples = comparison[comparison['changed']].head(10)\nfor _, row in changed_samples.iterrows():\n    print(f\"ID {row['ID']}: {row['old_pred']} → {row['new_pred']}\")\n\n# Checking that new model is confident in which of the classes\nprint(\"\\nClass changes summary:\")\nchange_summary = comparison[comparison['changed']].groupby(['old_pred', 'new_pred']).size()\nprint(change_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T13:04:30.921279Z","iopub.execute_input":"2025-06-10T13:04:30.921647Z","iopub.status.idle":"2025-06-10T13:04:30.937246Z","shell.execute_reply.started":"2025-06-10T13:04:30.921605Z","shell.execute_reply":"2025-06-10T13:04:30.936294Z"}},"outputs":[{"name":"stdout","text":"Sample prediction changes:\nID 1: forest → farm\nID 3: forest → orchard\nID 4: forest → farm\nID 5: forest → orchard\nID 6: forest → farm\nID 9: forest → farm\nID 10: forest → farm\nID 16: forest → farm\nID 17: forest → orchard\nID 19: forest → orchard\n\nClass changes summary:\nold_pred    new_pred  \nfarm        impervious      8\nforest      farm          425\n            grass         123\n            impervious    307\n            orchard         7\ngrass       impervious      1\nimpervious  water          13\norchard     impervious      1\n            water           1\nwater       farm           34\n            forest         10\n            impervious     87\ndtype: int64\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"print(\"=== MODEL COMPARISON ===\")\nprint(f\"Old submission score: 0.53136\")\nprint(f\"New model validation accuracy: {model.score(x_scaled, y_new):.5f}\")\nprint(f\"Predictions changed: {predictions_changed}/{total_predictions}\")\n\nif model.score(X_test_scaled, y_test) > 0.53136:\n    print(\"✅ Validation accuracy improved - good sign!\")\nelse:\n    print(\"⚠️  Validation accuracy not clearly better\")\n\nif predictions_changed > total_predictions * 0.1:  # More than 10% changed\n    print(\"✅ Significant prediction changes - could be much better!\")\nelif predictions_changed > 0:\n    print(\"✅ Some changes - might be slightly better\")\nelse:\n    print(\"❌ No changes - same score expected\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T13:06:30.960096Z","iopub.execute_input":"2025-06-10T13:06:30.960393Z","iopub.status.idle":"2025-06-10T13:06:30.970215Z","shell.execute_reply.started":"2025-06-10T13:06:30.960371Z","shell.execute_reply":"2025-06-10T13:06:30.969217Z"}},"outputs":[{"name":"stdout","text":"=== MODEL COMPARISON ===\nOld submission score: 0.53136\nNew model validation accuracy: 1.00000\nPredictions changed: 1017/2845\n✅ Validation accuracy improved - good sign!\n✅ Significant prediction changes - could be much better!\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}